1. Atomic operations:
Most architectures contain instructions that provide atomic versions of simple arithmetic operations; other architectures, lacking direct atomic operations, provide an operation to lock the memory bus for a single operation.
i)  Atomic integer operations:
============================
typedef struct{
	volatile int counter;
}atomic_t;
============================
Despite being an integer, and thus 32 bits on all the machines that Linux supports, develops and their code ONCE had to assume that an atomic_t was no larger than 24 bits in size. The SPARC port in Linux used the lower 8 bits of the 32 bits to implement lock. But now, such limitation is no more.

A common use of the atomic integer operations is to implement counters, using atomic operations is much lighter in weight than locks. Another use of the atomic integer operations is atomically performing an operation and testing the result. All the operations implemented on a specific architecture can be found in <asm/atomic.h>
ATOMIC_INIT(int i);                            At declaration, initialize to i
int atomic_read(atomic_t* v);                  Atomically read the integer value of v
void atomic_set(atomic_t* v, int i);           Atomically set v equal to i
void atomic_add(int i, atomic_t* v);           Atomically add i to v
void atomic_sub(int i, atomic_t* v);           
void atomic_inc(atomic_t* v);                  
void atomic_dec(atomic_t* v);                  
int atomic_sub_and_test(int i, atomic_t* v);   Atomically subtract i from v and return true if the result is 0, otherwise return false
int atomic_add_negative(int i, atomic_t* v);   Atomically add i to v and return true if the result is negative, otherwise return false
int atomic_add_return(int i, atomic_t* v);     Atomically add i to v and return the result
int atomic_sub_return(int i, atomic_t* v);     
int atomic_inc_return(int i, atomic_t* v);
int atomic_dec_return(int i, atomic_t* v);
int atomic_dec_and_test(atomic_t* v);          Atomically decrement v by 1 and return true if the result is 0, otherwise return false
int atomic_inc_and_test(atomic_t* v);          Atomically increment v by 1 and return true if the result is 0, otherwise return false

Atomicity V.S. Ordering:
Atomicity: happens all or none.
Ordering: is enforced via barrier operations

In the code it is usually preferred to choose atomic operations over more complicated locking mechanisms. On most architectures, 1 or 2 atomic operations incur less overhead and less cache-line thrashing than a more complicated synchronization method. As with performance-sensitive code, testing multiple approaches is always smart.

64-bit atomic operations:
Note that the size of atomic_t cannot change between architectures, so atomic_t is 32-bit even on 64-bit architectures.
============================
typedef struct{
	volatile long counter;
}atomic64_t;
============================
64-bit Atomic integer operations have the similar APIs as 32-bit, just change the prefix from atomic_ to atomic64_ .
For portability between all Linux's supported architectures, developers should use the 32-bit type atomic_t; the 64-bit type atomic64_t is reserved for code that is both architecture-specific and that requires 64-bits.

ii) Atomic bitwise operations:
Atomic bitwise operations are also architecture-specific and defined in <asm/bitops.h>
The bitwise functions operate on generic memory addresses. The arguments are a pointer and a bit number. Bit 0 is the LSB, and on a 32-bit machine, bit 31 is the MSB, and bit 32 is the LSB of next word.
void set_bit(int nr, void* addr);               Atomically set the nr-th bit starting from addr.
void clear_bit(int nr, void* addr);             Atomically clear the nr-th bit starting from addr.
void change_bit(int nr, void* addr);            Atomically flip the value of nr-th bit starting from addr.
int test_and_set_bit(int nr, void* addr);       Atomically set the nr-th bit starting from addr and return the previous value.
int test_and_clear_bit(int nr, void* addr);     Atomically clear the nr-th bit starting from addr and return the previous value.
int test_and_change_bit(int nr, void* addr);    Atomically flip the nr-th bit starting from addr and return the previous value.
int test_bit(void* addr);                       Atomically return the value of the nr-th bit starting from addr.

Nonatomic versions of all the bitwise functions are also provided. Just add 2 underscores "__" before the atomic versions.

The kernel also provides routines to find the first set/unset bit starting at a given address:
int find_first_bit(unsigned long* addr, unsigned int size);
int find_first_zero_bit(unsigned long* addr, unsigned int size);
Both functions take a pointer as their first argument and the number of bits in total to search as their second. They return the bit number of the first set or first unset bit, respectively. If just need to search only a word, the functions __ffs() and __ffz() are optimal, they just have 1 argument, the word in which to search.

2. Spin locks:
Spin lock is a lock that can be held by at most 1 thread of execution. If a spin lock is contended, i.e. trying to acquire a spin lock that is already held, then the thread busy loops, spins, (wasting processor time) waiting for the lock to become available.
The key point of spin lock is that holding the lock for a short period of time. It's wise to hold the spin locks for less than the duration of 2 context switches, or just try to hold the spin locks for as little time as possible. (An alternative behavior of when the lock is contended is to put the concurrent thread to sleep and wake it up when it becomes available. Such kind of lock is called semaphore.)
• Spin lock methods:
Spin locks are architecture-dependent and implemented in assembly. The architecture-dependent code is defined in <asm/spinlock.h>, the actual useful interfaces are defined in <linux/spinlock.h>. 

The lock can be held simultaneously by at most only 1 thread of execution, this provides the needed protection from concurrency on multiprocessing machines. On uniprocessor machines, spin locks are compiled away and do not exist; they simply act as markers to disable and enable kernel preemption. If kernel preemption is disabled, the lock is compiled away entirely.

Linux spin locks are not recursive.

Spin locks can be used in interrupt handlers, whereas semaphores cannot be used because they sleep. If a spin lock is used in an interrupt handler, you must also disable local interrupts before obtaining the lock. (Otherwise it is possible for an interrupt handler to interrupt kernel code while the lock is held and attempt to reacquire the lock. In such condition, it will deadlock forever.) But note that you only need to disable interrupts on the current processor. Because if an interrupt occurs on a different processor, and it spins on the same lock, it doesn't prevent the lock from being released eventually.

The kernel also provides an interface that conveniently disables interrupts and acquires the lock:
==========================================
DEFINE_SPINLOCK(mr_lock);
unsigned long flags;

spin_lock_irqsave(&mr_lock, flags);
/* critical region... */
spin_unlock_irqrestore(&mr_lock, flags);
==========================================
The routine spin_lock_irqsave(): i) saves the current state of interrupts; ii) disables them locally; iii) obtain the given lock. Conversely, spin_unlock_irqrestore() unlocks the given lock and returns interrupts to their previous state. This way, if interrupts were initially disabled, your code would not erroneously enable them, but instead keep them disabled. Note that the flags variable is seemingly passed by value, this is because the lock routines are implemented partially as macros.

On UP systems, the previous example must still disable interrupts to prevent an interrupt handler from accessing the shared data, but the lock mechanism is compiled away. The lock and unlock also disable and enable kernel preemption, respectively.

Lock data, not code!

Spin lock methods:
spin_lock();                  acquire given lock
spin_lock_irq();              disable local interrupts and acquire given lock
spin_lock_irqsave();          save current state of local interrupts, disables local interrupts, and acquires the given lock
spin_unlock();                release given lock
spin_unlock_irq();            release given lock and enable local interrupts
spin_unlock_irqrestore();     release given lock and restores local interrupts to the given previous state
spin_lock_init();             dynamically initializes given spinlock_t
spin_trylock();               try to acquire the given spin lock; if unavailable return nonzero
spin_is_locked();             return nonzero if the given lock is currently acquired, otherwise returns 0

• Spin locks and bottom halves:
Certain locking precautions must be taken when working with bottom halves. spin_lock_bh() obtains the given lock and disables all the bottom halves. spin_unlock_bh() performs the inverse.
Because bottom halves might preempt process context code (like work queues), so if data is shared between a bottom half and process context, you must protect the data in process context with both a spin lock and disabling of bottom halves. Likewise, because an interrupt handler might preempt a bottom half, if data is shared between an interrupt handler and a bottom half, you must obtain the appropriate lock and disable interrupts.

Since two tasklets of the same type cannot run simultaneously on different processors, so there's no need to protect data used only within a single type of tasklet. If the data is shared between two different tasklets, you must obtain a normal spin lock before accessing the data in bottom half. You do not need to disable bottom halves because a tasklet never preempts another tasklet running on the same processor.

And with softirqs, regardless of whether it is the same type of softirqs, if data is shared by softirqs, it must be protected with a lock. A softirq never preempts another softirq, so there's no need to disable bottom halves. (The only event that can preempt a softirq is an interrupt handler.)

• Reader-writer spin locks:
Reader-writer spin locks provide separate reader and writer variants of the lock. Usage is similar to spin locks:
============================================
DEACARE_RWLOCK(mr_rwlock);
Then, in the reader code path:
read_lock(&mr_lock);
/* critical section (read only)... */
read_unlock(&mr_lock);
In the writer code path:
write_lock(&mr_rwlock);
/* critical section (read and write)... */
write_unlock(&mr_rwlock);
============================================
Note that you cannot "upgrade" a read lock to a write lock. For example, this code snippet:
========================
read_lock(&mr_rwlock);
write_lock(&mr_rwlock);
========================
Executing these two functions as shown above will deadlock. Because as the writer spins (waiting for all the readers release the lock), it will never end. If you ever need to write, obtain the write lock from the start. If the line between your readers and writers is muddled, it might be an indication that you do need to use reader-writer locks. In this case, a normal spin lock is optimal.
It is safe for multiple readers to obtain the same lock. In fact, it is safe for the same thread to recursively obtain the same read lock. If you have only readers in interrupt handlers but no writers, you can mix the use of the "interrupt disabling" locks. (read_lock_irq()) You can use read_lock() instead of read_lock_irqsave() for reader protection. You will still need to disable interrupts for write access, i.e. write_lock_irqsave(), otherwise a reader in an interrupt could deadlock on the held write lock.

read_lock();                 
read_lock_irq();             Disables local interrupts and acquires given lock for reading
read_lock_irqsave();         Saves the current state of local interrupts, disables local interrupts, and acquires the given lock for reading
read_unlock();
read_unlock_irq();           Releases given lock and enables local interrupts
read_unlock_irqrestore();    Releases given lock and restores local interrupts to the given previous state
write_lock();
write_lock_irq();            
write_lock_irqsave();
write_unlock();
write_unlock_irq();
write_unlock_irqrestore();
write_trylock();             Tries to acquire given lock for writing; if unavailable, returns nonzero
rwlock_init();               Initializes given rwlock_t

• A final important consideration in using the linux reader-writer spin locks is that they favor readers over writers. 
Spin locks provide a quick and simple lock. The spinning behavior is optimal for short hold times and code that cannot sleep (interrupt handlers, for example.) In case where the sleep time might be long or you potentially need to sleep while holding the lock, the semaphore is a solution.

3. Semaphores:
Semaphores in Linux are sleeping locks. When a task attempts to acquire a semaphore that is unavailable, the semaphore places the task onto a wait queue and puts the task to sleep. The processor is then free to execute other code. When the semaphore becomes available, one of the tasks on the wait queue is awakened so that it can then acquire the semaphore. This provides better processor utilization than spin locks, but semaphores have much greater overhead than spin locks.

• Semaphores are well suited to locks that are held for a long time. And because a thread of execution sleeps on lock contention, semaphores must be obtained only in process context because interrupt context is not schedulable.
• You can (although you might not want to) sleep while holding a semaphore because you will not deadlock when another process acquires the same semaphore. (It will just go to sleep and eventually let you continue.)
• You cannot hold a spin lock while you acquire a semaphore. Because you might have to sleep while waiting for the semaphore, and you cannot sleep while holding a spin lock.
• If your code needs to sleep, which is often the case when synchronization with user-space, semaphore is the only solution. But if you do have a choice, the decision between semaphore and spin lock should be based on lock hold time. Additionally, unlike spin locks, semaphores do not disable kernel preemption, so consequently, code holding a semaphore can be preempted. This means semaphores do not adversely affect scheduling latency.

• Creating and Initializing semaphores:
The semaphore implementation is architecture-dependent and defined in <asm/semaphore.h>. The struct semaphore type represents semaphores:
=========================
struct semaphore name;
sema_init(&name, count);   // initialize the counting semaphore to count
=========================
A more common use to create a mutex (with count == 1) is as follows:
============================
static DECLARE_MUTEX(name); // name is the variable name of the mutex (binary semaphore)
============================
To initialize a dynamically created mutex, you can:
=================
init_MUTEX(sem);
=================

• Using semaphores:
down_interruptible() (like P() operation) attempts to acquire the given semaphore, if it is unavailable, it places the calling process to sleep in the TASK_INTERRUPTIBLE state. (it could be waken up by a signal.) If the task receives a signal while waiting for the semaphore, it is awakened and down_interruptible() returns -EINTR. 
Alternatively, the function down() places the task in the TASK_UNINTERRUPTIBLE state when it sleeps. You can use down_trylock() to try to acquire the given semaphores without blocking. If the semaphore is already held, the function immediately returns nonzero. Otherwise, it returns 0 and you successfully hold the lock.

sema_init(struct semaphore*, int);           Initializes the dynamically created semaphore to the given count.
init_MUTEX(struct semaphore*);               Initializes the dynamically created semaphore with the count of 1.
init_MUTEX_LOCKED(struct semaphore*);        Initializes the dynamically created semaphore with a count of 0. (So it is initially locked.)
down_interruptible(struct semaphore*);       Tries to acquire the given semaphore and enter interruptible sleep if it is contended.
down(struct semaphore*);                     Tries to acquire the given semaphore and enter uninterruptible sleep if it is contended.
down_trylock(struct semaphore*);             Tries to acquire the given semaphore and immediately return nonzero if it is contended.
up(struct semaphore*);                       Releases the given semaphore and wakes a waiting task, if any.

• Reader-writer semaphores:
Reader-writer semaphores created dynamically are initialized via:
======================================
init_rwsem(struct rw_semaphore* sem);
======================================
All reader-writer semaphores are mutexes! Although they enforce mutual exclusion only for writers, not readers. And all reader-writer locks use uninterruptible sleep, so there's only one version of each down(). For example:
===========================================
static DECLARE_RWSEM(mr_rwsem);
down_read(&mr_rwsem);
/* critical section, read only... */
up_read(&mr_rwsem);

....
down_write(&mr_rwsem);
/* critical section, read and write... */
up_write(&mr_sem);
===========================================
• down_read_trylock() and down_write_trylock(): they both return nonzero if lock is successfully acquired and 0 if it is currently contended. This is the opposite of the normal behavior (return value)!
• Reader-writer semaphores have a unique method that their reader-writer spin lock cousins do not have: downgrade_write(). This function atomically converts an acquired write lock to a read lock.

4. Mutexes
Semaphores are rather generic and do not impose many usage constrains. This makes them useful for managing exclusive access in obscure situations, such as complicated dances between the kernel and user space.
Now linux kernel implements mutex, represented by struct mutex. It is similar to a semaphore with count of 1, but it has a simpler interface, more efficient performance, and additional constrains on its use.
DEFINE_MUTEX(name);                To statically define a mutex
mutex_init(struct mutex*);         To dynamically initialize a mutex
mutex_lock(struct mutex*);         Locks the given mutex; sleeps if the lock is contended
mutex_unlock(struct mutex*);        
mutex_trylock(struct mutex*);      Tries to acquire the given mutex; returns 1 if the lock is successfully acquired, and 0 otherwise.
mutex_is_locked(struct mutex*);    Returns 1 if the lock is locked and 0 otherwise.

• The simplicity and efficiency of the mutex comes from the additional constrains it imposes on its users. Mutex has a stricter, narrower use case:
i)   Whoever locks the mutex must unlock it. That is, you cannot lock a mutex in one context and then unlock it in another. This means that the mutex isn't suitable for more complicated synchronizations between kernel and user-space. (But most use cases, cleanly lock and unlock from the same context.)
ii)  Recursive locks and unlocks are not allowed. That is, you cannot recursively acquire the same mutex; and you cannot unlock an unlocked mutex.
iii) A process cannot exit while holding a mutex.
iv)  A mutex cannot be acquired by an interrupt handler or bottom half, even with mutex_trylock().
v)   A mutex can be managed only via the official API: it must be initialized via the methods described in this section and cannot be copied, hand initialized, or reinitialized.

When the kernel configuration option CONFIG_DEBUG_MUTEXES is enabled, a multitude of debugging checks ensure theses constrains are always upheld. CONFIG_DEBUG_SPINLOCK enables a handful of debugging checks in the spin lock code. For additional debugging of lock lifecycles, enable CONFIG_DEBUG_LOCK_ALLOC.
Unless one of mutex's additional constrains prevent you from using them, prefer to use the new mutex type to semaphores with count of 1.

5. Completion variables:
Using completion variables is an easy way to synchronize between 2 tasks in the kernel when one task needs to signal the other that one event has occured. One task waits on the completion variable while another task performs some work. When the other task has completed the work, it uses the completion variable to wake up any waiting tasks. (It is somewhat like condition variables, but it's much easier to use and doesn't require the mutex.)
Completion variables are represented by the struct completion type, which is defined in <linux/completion.h>

DECLARE_COMPLETION(mr_comp);                Statically create a completion variable
init_completion(struct completion*);        Initializes the given completion variable
wait_for_completion(struct completion*);    Waits for the given completion variable to be signaled
complete(struct completion*);               Signals any waiting tasks to wake up

For sample usages of completion variables, see kernel/sched.c and kernel/fork.c. A common usage is to have a completion variable dynamically created as a member of data structure. Kernel code waiting for the initialization of the data structure calls wait_for_completion(). When the initialization is complete, the waiting tasks are awakened via a call to complete().

6. BKL: the Big Kernel Lock
BKL is a global spin lock that was created to ease the transition from Linux's original SMP implementation to fine-grained locking. The BKL has some interesting properties:
• You can sleep while hoding the BKL. (This is different from normal spin locks) The lock is automatically dropped when the task is unscheduled and reacquired when the task is rescheduled. (It is somewhat like condition variables again...) This does not mean it is always safe to sleep while holding the BKL, merely that you can and you won't deadlock.
• The BKL is a recursive lock. A single process could acquire the lock multiple times and not deadlock, this is different from normal spin locks.
• You can use BKL only in process context. Unlike spin locks, you cannot acquire the BKL in interrupt context.
• New users of the BKL are forbidden. With every kernel release, fewer and fewer drivers and subsystems rely on the BKL.

The following interfaces are declared in <linux/smp_lock.h>:
lock_kernel();           Acquires the BKL
unlock_kernel();         Releases the BKL
kernel_locked();         Returns nonzero if the lock is held, and 0 otherwise. (UP always returns nonzero.)

• The BKL also disables kernel preemption while it is held. On UP kernels, the BKL code doesn't actually perform any physical locking.
• One of the major issues concerning the BKL is determining what the lock is protecting. Too often the BKL is seemingly associated with code instead of data. This makes replacing BKL uses with a spin lock difficult because it is not easy to determine just what is being locked.

7. Sequential locks: (http://en.wikipedia.org/wiki/Seqlock)
The sequential lock is a new type of lock introduced in the 2.6 kernel. It provides a simple mechanism for reading and writing shared data. It works by maintaining a sequence counter. Whenever the data in question is writing to, a lock is obtained and a sequence number is incremented. (Writer increments the sequence number, both after acquiring the lock and before releasing the lock. http://en.wikipedia.org/wiki/Seqlock) Prior to and after reading the data, the sequence number is read; if the values are the same, a write didn't begin in the middle of read. Further, if the values are even, a write is not underway. (Grabbing the write lock makes the value odd, whereas releasing it makes it even because the lock starts at 0.)
=================================================================================
To define a seq lock:
seqlock_t mr_seq_lock = DEFINE_SEQLOCK(mr_seq_lock);

The write path is then:
write_seqlock(&mr_seq_lock);
/* write lock is obtained... */
write_sequnlock(&mr_seq_lock);

This looks like normal spin lock code. The oddness comes in with the read path:

unsigned long seq;
do{
	seq = read_seqbegin(&mr_seq_lock);
	/* read data here... */
}while(read_seqretry(&mr_seq_lock, seq));
=================================================================================
Seq locks are useful to provide a lightweight and scalable lock for use with many readers and a few writers. Seq locks, however, favor writers over readers. Seq locks are ideal when your locking needs meet most or all of the following requirements:
• Your data has a lot of readers. Your data has few writers
• Although few in number, you want to favor writers over readers and never allow readers to starve writers.
• Your data is simple, such as a simple structure or even a single integer that, for whatever reason, cannot be made atomic.

A prominent use of the seq locks is jiffies, the variable that stores a linux machine's uptime. Jiffies holds a 64-bit count of the number of clock ticks since the machine booted. On machines that cannot atomically read the full 64-bit jiffies_64 variable, get_jiffies_64() is implemented using locks:
=============================================
u64 get_jiffies_64(void)
{
	unsigned long seq;
	u64 ret;
	do{
		seq = read_seqbegin(&xtime_lock);
		ret = jiffies_64;
	}while(read_seqretry(&xtime_lock, seq));
	return ret;
}
=============================================
Updating jiffies during timer interrupt, in turns, grabs the write variant of the seq lock:
===============================
write_seqlock(&xtime_lock);
jiffies_64 += 1;
write_sequnlock(&xtime_lock);
===============================

* seqlock is more efficient than traditional read-write locks for the situation where there are many readers and few writers. The drawback is that if there is too much write activity or the reader is too slow, they might livelock. (The readers may starve.)
* Also note that seqlock will not work for data that contains pointers, because any writer may invalidate a pointer that a reader has already followed. In this case, using read-copy-update synchronization is preferred.
* One subtle issue of using seqlocks for a time counter is that it's impossible to step through it with a debugger. The read retry is triggered all the time because the debugger is slow enough.


8. Preemption disabling:
• Because the kernel is preemptive, a task can begin running in the same critical section as a task that was preempted. To prevent this, the kernel preemption uses spin locks as markers of nonpreemptive regions. If a spin lock is held, the kernel is not preemptive. (Because the concurrency issues with kernel preemption and SMP are the same, and the kernel is SMP-safe using spin locks, so this simple change also makes the kernel preempt-safe.) For more information, see the Linux spinlock's implementation. preempt_disable() is always disabled.
But some situations do not require a spin lock, but do need kernel preemption disabled. The scenario is like per-processor data. Since the data is unique to each processor, so it doesn't need to protect it with a spin lock. But if no spin locks are held, the kernel is preemptive, and it would be possible for a newly scheduled task to access this same data. Consequently, even if this were a UP system, the variable could be accessed pseudo-concurrently by multiple processes. Normally this variable would require a spin lock to prevent true concurrency on multiprocessing machines. If this were a per-processor variable, it might not require a lock.

To solve this, kernel preemption can be disabled via preempt_disable(). This function is nestable, the final corresponding call to preempt_enable() reenables preemption. The preemption count stores the number of held locks, if it is greater than 0, then kernel is not preemptible.

preempt_disable();                 Disables kernel preemption by increasing the preemption counter.
preempt_enable();                  Decrements the preemption counter and checks any pending reschedules if the new count is 0.
preempt_enable_no_resched();       Enables kernel preemption but does not check for any pending reschedules.
preempt_count();                   Returns the preemption count.

• As a cleaner solution to per-processor data issues, you can obtain the processor number(index) via get_cpu(). It disables kernel preemption prior to returning the current processor number.
=========================================================================
int cpu;
/* disable kernel preemption and set cpu to the current processor */
cpu = get_cpu();

/* manipulate per-processor data... */

/* reenable kernel preemption, cpu can change so is no longer valid */
put_cpu();
=========================================================================

9. Ordering and barriers:
All processors that do reorder reads or writes provide machine instructions to enforce ordering requirements. It is also possible to instruct the compiler not to reorder instructions around a given point. These instructions are called barriers.

rmb() method provides a read memory barrier. It ensures that no loads are reordered across the rmb() call. That is, no loads prior to the call will be reordered to after the call. And no loads after the call will be reordered to before the call.
wmb() provides a write barrier.
mb() method provides both a read barrier and a write barrier. No loads or stores will be reordered across a call to mb().
A variant of rmb(), read_barrier_depends(), provides a read barrier but only for loads on which subsequent loads depend. All reads prior to the barrier are guaranteed to complete before any reads after the barrier that depend on the reads prior to the barrier. On some architectures, read_barrier_depends() is much quicker than rmb() because it is not needed and is, thus, a nop.

The macros smp_rmb(), smp_wmb(), and smp_read_depends() provide a useful optimization. On SMP kernels they're defined as memory barriers, whereas on UP kernels they're defined only as compiler barriers. 
The barrier() method prevents the compiler from optimizing loads or stores across the call. The previous memory barriers also function as compiler barriers, but a compiler barrier is much lighter than memory barrier. Indeed, a compiler barrier is practically free, because it simply prevents the compiler from possibly rearranging things.

Note that the actual effects of the barriers vary for each architecture. For example, if a machine doesn't perform out-of-order stores (e.x. x86 doesn't), wmb() does nothing.

rmb();                          Prevents loads from being reordered acrossed the barrier.
read_barrier_depends();         Prevents data-dependent loads from being reordered across the barrier.
wmb();                          ........ stores ..........
mb();                           ........ loads and stores ........
smp_rmb();                      Provides a rmb() on SMP, and on UP provides a barrier()
smp_read_barrier_depends();     Provides a read_barrier_depend() on SMP, and on UP provides a barrier()
smp_wmb();                      Provides a wmb() on SMP, and on UP provides a barrier()
smp_mb();                       .........  mp() ....................................
barrier();                      Provides a compiler barrier: prevents the compiler from optimizing stores or loads across the barrier.
