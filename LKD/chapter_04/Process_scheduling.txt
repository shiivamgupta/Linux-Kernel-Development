1. Multitasking:
Multitasking operating systems come in two flavors: cooperative multitasking and preemptive multitasking.
• In preemptive multitasking, the scheduler decides when a process is to cease running and a new process is to begin running. The act of involuntarily suspending process is called preemption.
• In cooperative multitasking, a process doesn't stop running until it voluntary decides to do so. The act of a process voluntarily suspending itself is called yielding.

2. Linux's process scheduler:
O(1) scheduler, CFS(Complete Fair Scheduler).

3. Policy:
• I/O-bound v.s. processor-bound processes: Most GUI applications are also I/O bound processes. The scheduler policy for CPU-bound processes are: run such processes less frequently, but with for longer duration. Processes can be both I/O-bound and CPU-bound at the same time, like X window server, and word processer.
The scheduling policy must attempt to satisfy two conflicting goals: fast process response time (low latency) and maximal system utilization (high throughput).
• The scheduler policy in Unix systems explicitly favor I/O-bound processes, thus providing good process respond time.

4. Process priority:
• Nice value: -20 ~ +19, the larger the nice number, the lower the priority. Different unix systems apply nice values in different ways: in Mac OS X, the nice value is a control over the absolute timeslice allotted to a process; in Linux, it is the control over the proportion of timeslice. (ps -el could see nice value information of all processes.)
• Real-time priority: the higher the real-time priority, the higher the priority. All real-time processes are at a higher priority than normal processes. (i.e. real-time priority and nice value are in disjoint sets.) "ps -eo state,uid,pid,ppid,rtprio,time,comm" could see real-time priority (RTPRIO column, a value of "-" means the process is not real-time.)

5. Timeslice:
CFS assigns processes a proportion of the processor. The decision is a function of how much of a proportion of the processor the newly runnable process has consumed. If it has consumed a smaller proportion of the processor than the currently executing process, it runs immediately, preempting the current process. If not, it is scheduled to run at a later time.

6. The linux scheduling algorithm:
• Scheduler classes:
The linux scheduler is a modular enabling different algorithms to schedule different types of processes. This modularity is called scheduler classes. Each scheduler class has a priority. The base scheduler code, exists in kernel/sched.c, iterates over each scheduler class in order of priority. The highest priority scheduler class that has a runnable process wins, selecting who runs next.
CFS is the registered scheduler class for normal processes, called SCHED_NORMAL in Linux. CFS is defined in kernel/sched_fair.c. Instead of using the nice value to calculate a timeslice, CFS uses nice value to weight the proportion of processor a process is to receive. CFS also has a minimum granularity, by default it is 1 millisecond.

7. The linux scheduling implementation:
Four components of CFS: 1. Time accounting; 2. Process selection; 3. The scheduler entry point; 4. Sleeping and waking up.
• Time accounting:
All process schedulers must account for the time that a process runs.
The scheduler entity structure: defined in <linux/sched.h>, struct sched_entity, this structure is also embedded in process descriptor, named se.
The virtual runtime: the vruntime variable stores the virtual runtime of a process, which is the actual runtime (the amount of time spent running) normalized by the number of runnable processes. The virtual runtime's units are nanoseconds and therefore vruntime is decoupled from the timer tick. On an ideal processor, the virtual runtime of all processes would be identical, because processors are not capable of perfect multitasking and we must run each process in succession, CFS uses vruntime to account for how long a process has run.

• Process selection:
CFS attempts to balance a process's virtual runtime with a simple rule: CFS picks the process with the smallest vruntime.(This is the core part of CFS.)
CFS uses a red-black tree to manage the list of runnable processes, thus finding the task with the smallest vruntime efficiently. (The node contains the smallest vruntime is the leftmost node of the red-black tree, but it's also cached in the rb_leftmost.) If here're no runnable processes, CFS schedules the idle task. rb_leftmost should be updated when a task becomes runnable (wake up) or it's first created by fork. Since the red-black tree is for the runnable processes, so when a process terminates or blocks(becomes unrunnable), it should be removed from the tree.

• The scheduler entry point:
The main entry point into the process schedule is the function schedule(), defined in kernel/sched.c, it finds the highest priority scheduler class with a runnable process and asks it what to run next.

• Sleeping and waking up:
The task marks itself as sleeping; put itself on a wait queue; remove itself from the red-black tree representing the runnable processes; calls schedule() to select a new process to execute. Waking up is the inverse: the task is set as runnable, removed from the wait queue, added back to the red-black tree.
Two states are associated with sleeping: TASK_INTERRUPTIBLE and TASK_UNINTERRUPTIBLE, both are sit in a wait queue.

• Wait queues:
Wait queues in the kernel are represented in the kernel by wake_queue_head_t. Wait queues are created statically via DECLARE_WAITQUEUE(), or dynamically via init_waitqueue_head(). 
======================================================================
/* 'q' is the wait queue we wish to sleep on */
	DEFINE_WAIT(wait);
	
	add_wait_queue(q, &wait);
	while(!condition)  // condition is the event we're waiting for
	{
		prepare_to_wait(&q, &wait, TASK_INTERRUPTIBLE);
		if(signal_pending(current))
			/* handle signal */
		schedule();
	}
	finish_wait(&q, &wait);
======================================================================
The task performs the following steps to add itself to a wait queue:
i)   Create a wait queue entry via macro DEFINE_WAIT()
ii)  Add itself to a wait queue via add_wait_queue(). 
iii) Call prepare_to_wait() to change the process's state to either TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE.
iv)  If the state is set to TASK_INTERRUPTIBLE, and a signal wakes it up (a spurious wake up), needs to check and handle signals.
v)   When the tasks wakes up, should check again whether the condition is met or not, to avoid TOCTTOU bug. So we should use while() to check conditions.
vi)  Now that the condition is true, the task set itself to TASK_RUNNING and removes itself from the wait queue via finish_wait().

• Waking up:
Waking is handled via wake_up(), which wakes up all the tasks waiting on the given wait queue. It calls try_to_wake_up() to set the task's state to TASK_RUNNING; calls enqueue_task() to add the task to the red-black tree; and sets need_resched if the awakened task's priority is higher than the priority of current task.

8. Preemption and context switching:
context_switch() is in the kernel/sched.c, it is called by schedule() when a new process has been selected to run. It does 2 things:
• Calls switch_mm() (in <asm/mm_context.h>) to switch the virtual memory mapping from the previous process's to the new process's.
• Calls switch_to() (in <asm/system.h>) to switch the processor state from the previous process's to the new process's, including saving and restoring the stack information and registers and other architecture-specific state.

The kernel needs to know when to call schedule(), a flag need_resched is used to signify whether a reschedule should be performed. This flag is set by 1. scheduler_tick() when a process should be preempted; 2. by try_to_wake_up() when a process that has a higher priority than the current process is awakened. This flag is a message to the kernel that the scheduler should be invoked as soon as possible.
Upon returning to user-space or returning from an interrupt, the need_resched flag is checked. If it's set, kernel invokes schedule() before continuting.

• User preemption:
User preemption occurs when the kernel is about to return to user-space. Because if the kernel is returning to the user-space, then it's safe to continue executing the current task, it's also safe to pick a new task to execute. So whenever kernel is about to return to user-space either on returning from an interrupt handler or after a syscall, need_resched is checked. 
Both the return paths from interrupt handler or syscall are architecture-dependent and typically implemented in assembly in entry.S.
In short, user preemption can occur: 
i)  When returning to user-space from a syscall;
ii) When returning to user-space from an interrupt handler.

• Kernel preemption:
Linux kernel is a fully preemptive kernel. In non-preemptive kernels, kernel code runs until completion. So the scheduler can't rescuedule a task while it's in kernel-space, i.e. kernel code is scheduled cooperatively. But since 2.6, it's now possible to preempt a task at any point, so long as the kernel is in a "safe" state.
The kernel can preempt a task running in the kernel so long as it doesn't hold a lock. i.e., locks are used as markers of regions of nonpreemptibility.
Each process has a variable preempt_count in thread_info structure. This variable begins from 0, and increments by 1 each time the process acquires a lock, and decrements by 1 once it releases a lock. When the count is 0, the kernel is preemptible. 
Upon returning from interrupt, if returning to kernel-space, the kernel checks preempt_count and need_resched:
i)   If need_resched is set and preempt_count is 0, then it's safe to preempt, schedule() is invoked;
ii)  If preempt_count is non-zero, the interrupt returns as usual to the currently executing task.
iii) When all the locks currently running task holds are released, preempt_count -> 0, at this point, the unlock code checks whether need_resched is set, if so, the scheduler is invoked. (Enabling and disabling kernel preemption is sometimes required in kernel code, which is discussed in Chapter 9.)

Kernel preemption can also occur explicitly, when a task in the kernel blocks or explicitly calls schedule(). It is assumed that the code that explicitly calls schedule() knows it is safe to reschedule.

In short, kernel preemption can occur:
i)   When an interrupt handler exits, before returning to kernel-space;
ii)  When kernel code becomes preemptible again; (preempt_count -> 0)
iii) If a task in the kernel explicitly calls schedule();
iv)  If a task in the kernel blocks (which results in a call to schedule()).

9. Real-time scheduling policies:
Linux provides 2 real-time scheduling policies: SCHED_FIFO, and SCHED_RR. (Normal, not real-time scheduling policy is SCHED_NORMAL.) These 2 real-time scheduling policies are not managed by CFS, but by a special real-time scheduler, defined in kernel/sched_rt.c.
• SCHED_FIFO:
Implements a simple first-in-first-out scheduling algorithm without timeslices. A runnable SCHED_FIFO task is always scheduled over any SCHED_NORMAL tasks. When a SCHED_FIFO task becomes runnable, it continues running until it blocks or explicitly yields the processor, it has no timeslice so can run indefinitely. Only a higher priority SCHED_FIFO or SCHED_RR tasks can preempt a SCHED_TASK task. Two or more SCHED_FIFO tasks with the same priority run round-robin, but again only yielding the processor when they explicitly choose to do so. 
• SCHED_RR:
SCHED_RR is basically identical to SCHED_FIFO except that each process can run only until it exhausts a predetermined timeslice. i.e., SCHED_RR is SCHED_FIFO with a timeslice. When a SCHED_RR task exhausts its timeslice, any other real-time processes at its priority are scheduled round-robin. As with SCHED_FIFO, a higher-priority process always preempts a lower-priority one, but a lower-priority process can never preempt a SCHED_RR process, even if its timeslice is exhausted.

Both real-time scheduling policies in Linux provide soft real-time behavior. Soft real-time behavior refers to the notion that the kernel tries to schedule applications within timing deadlines, but kernel doesn't promise to always achieve these goals. But the real-time performance of linux is quite good.

10. Scheduler-related system calls:
i)   Scheduling policy and priority-related system calls:
nice(): for normal tasks, nice() increments the given process's static priority by a given position amount; only root can provide a negative number.
sched_setscheduler(): set a process's scheduling policy;
sched_getscheduler(): get ....;
sched_setparam(): set a process's real-time priority;
sched_getparam(): get ....;
sched_get_priority_max(): get the maximum real-time priority;
sched_get_priority_min(): get the minimum ....;
sched_rr_get_interval(): get a process's timeslice value;

ii)  Processor affinity system calls:
The kernel could enforce hard affinity of a process (the process could only run on a specified processor). This hard affinity is stored as a bitmask in the task's task_struct as cpus_allowed.
Sched_setaffinity(): set a process's processor affinity;
sched_getaffinity(): get ....;

iii) Yielding processor time:
Linux provides sched_yield() syscall for a process to explicitly yield the processor to other waiting processes. It works by removing the process from the active array (where it currently is, because it is running) and inserting it into the expired array. This has the effect of not only preempting the process and put it at the end of its priority list, but also putting it on the expired list, thus guaranteeing it won't run for a while. (Because real time tasks never expire, they're a special case, just move them to the end of their priority list. (and not insert them into the expired array.)) 

Kernel code, as a convenience, can call yield(), which ensures the task's state is TASK_RUNNING and then call sched_yield(); user-space applications use sched_yield() syscall.
