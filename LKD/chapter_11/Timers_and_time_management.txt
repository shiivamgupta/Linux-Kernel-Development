• i)  Events that occur periodically, say, every 10ms, are driven by the system timer. System timer is a programmable piece of hardware that issues an interrupt at a fixed frequency. The interrupt handler for this timer - called timer interrupt - updates the system time and performs periodic work. The system timer and its timer interrupt are central to Linux and a large focus of this chapter.
  ii) Dynamic timers, the facility used to schedule events that run once after a specified time has elapsed. Kernel can create and destroy such timers dynamically.

1. Kernel notion of time:
Kernel keeps track of both wall time and system uptime. Wall time, the actual time of day, is important to user-space applications. The system uptime, the relative time since the system booted, is useful to both kernel space and user space.
Timer interrupt is importmant to the management of the operating system. Some of the work executed periodically by the timer interrupt includes:
i)   Updating the system uptime
ii)  Updating the time of day
iii) On an SMP system, ensuring that the scheduler runqueues are balanced and, if not, balancing them
iv)  Running any dynamic timers that have expired
v)   Updating resource usage and processor time statistics.

• The tick rate: HZ
The frequency of system timer (the tick rate) is programmed on system boot based on a static preprocessor define, HZ. The value of HZ differs for each supported architecture. The kernel defines the value in <asm/param.h>. The tick rate has a frequency of HZ hertz. When writing kernel code, never assume that HZ has any given value.

HZ is now a configuration option, although the most common value is 100 or 1000. Increasing the tick rate means the timer interrupt runs more frequently. This has the following benefits:
i)  The timer interrupt has a higher resolution, and consequently, all timed events have a higher resolution.
ii) The accuracy of timed events improves.
Assuming the kernel starts timers at random times, the average timer is off by half the period of the timer interrupt because timers might expire at any time, but are executed only on occurrences of the timer interrupt.

• Advantages with a larger HZ:
i)   Kernel timers execute with finer resolution, and increased accuracy.
ii)  System calls such as poll() and select() that optionally employ a timeout value execute with improved precision.
iii) Measurements, such as resource usage or the system uptime, are recorded with a finer resolution.
iv)  Process preemption occurs more accurately, which results in decreased scheduling latency. Assuming a given process is running and has a2 milliseconds of its timeslice remaining. In 2 milliseconds, the scheduler SHOULD preempt the running process and begin executing a new process. Unfortunately, this event doesn't occur until the next timer interrupt, which might not be in 2 milliseconds. At worst the next timer interrupt might be 1/HZ of a second away. Of course, this all balances out and fairness is preserved, because all tasks receive the same imprecision in scheduling - but that is not the issue. The problem stems from the latency created by the delayed preemption. If the to-be-scheduled task had something time-sensitive to do, such as refill an audio buffer, the delay might not be acceptable.

• Disadvantages with a larger HZ:
A higher tick rate implies more frequent timer interrupts, which implies higher overhead, because the processor must spend more time executing the timer interrupt. This adds up to not just less processor time available for other work, but also a more frequent thrashing of the processor's cache and increase in power consumption.

• A tickless OS:
Linux kernel supports an option known as a tickless operation. When a kernel is built with the CONFIG_HZ configuration option set, the system dynamically schedules the timer interrupt in accordance with pending timers. 

2. jiffies:
The jiffies variable is declared in <linux/jiffies.h>: 
extern unsigned long volatile jiffies;
It holds the number of ticks that have occurred since the system booted. On boot this variable is initialized to 0, and it is incremented by 1 during each timer interrupt, so the system uptime is jiffies/HZ seconds.

• The jiffies variable has always been an unsigned long, and therefore 32 bits in size on 32-bit architecture and 64 bits on 64-bit architecture. If jiffies is stored in 32 bits, an with HZ = 100, then jiffies will overflow in 497 days; if jiffies is stored in 64 bits, then for any reasonable HZ value the jiffies variable would never overflow in anyone's lifetime.

• Code that accesses jiffies simply reads the lower 32 bits of jiffies_64. The function get_jiffies_64() can be used to read the full 64-bit value. On 64-bit architectures, jiffies_64 and jiffies refer to the same thing.

• jiffies wraparound:
The kernel provides 4 macros for comparing tick counts that correctly handle wraparound in the tick count. They are in <linux/jiffies.h>:
#define time_after(unknown, known) ((long)(known) - (long)(unknown) < 0)
#define time_before(unknown, known) ((long)(unknown) - (long)(known) < 0)
#define time_after_eq(unknown, known) ((long)(unknown) - (long)(known) >= 0)
#define time_before_eq(unknown, known) ((long)(known) - (long)(unknown) >= 0)

The unknown parameter is typically jiffies and the known parameter is the value against which you want to compare. (Could search web or see P216 why these macro could prevent jiffies-wraparound.)

• User-space and HZ:
Changing HZ would scale various exported values by some constant - without user-space knowing! Uptime would read 20 hours when it was in fact 2! To prevent such problems, the kernel needs to scale all exported jiffies values. It does this by defining USER_HZ, which is the HZ value that user-space expects. The function jiffies_to_clock_t(), defined in kernel/time.c, is used to scale a tick count in terms of HZ to a tick count in terms of USER_HZ. See the source code for more details. A more complicated algorithm is used if the values are not integer multiples. Finally, the function jiffies_64_to_clock_t() is provided to convert a 64-bit jiffies value from HZ to USER_HZ units.

• Hardware clocks and timers:
Architectures provide 2 hardware devices to help with time keeping: the system timer and the real-time clock.
i)  Real-time clock (RTC):
RTC provides a nonvolatile device for storing the system time. On boot, the kernel reads the RTC and uses it to initialize the wall time, which is stored in xtime variable.
ii) System timer:
The idea behind the system timer, regardless of architecture, is the same - to provide a mechanism for driving an interrupt at a periodic rate. On x86, the primary system timer is the programmable interrupt timer (PIT). The kernel programs the PIT on boot to drive the system timer interrupt at HZ frequency.

3. The timer interrupt handler:
The timer interrupt is broken into 2 pieces: an architecture-dependent and an architecture-independant routine.
i)  The architecture-dependent routine is registered as the interrupt handler for the system timer. Its exact job depends on the given architecture, but most handlers perform at least the following work:
• Obtain the xtime_lock lock, which protects access to jiffies_64 and the wall time value - xtime.
• Acknowledge or reset the system timer as required.
• Periodically save the updated wall time to the real time clock (RTC).
• Call the architecture-independant timer routine, tick_periodic().

ii) The architecture-independent routine, tick_periodic() performs much work:
• Increment jiffies_64 by 1. (xtime_lock is already held)
• Update resource usages, like consumed system and user time for the current running process.
• Run any dynamic timers that have expired.
• Execute scheduler_tick().
• Update the wall time, which is stored in xtime.
• Calculate load average.
Code is as follows:
======================================================================
static void tick_periodic(int cpu)
{
	if(tick_do_timer_cpu == cpu)
	{
		write_seqlock(&xtime_lock);
		/* keep track of the next tick event */
		tick_next_period = ktime_add(tick_next_period, tick_period);
		do_timer(1);
		write_sequnlock(&xtime_lock);
	}
	update_process_times(user_mode(get_irq_regs()));
	profile_tick(CPU_PROFILING);
}

void do_timer(unsigned long ticks)
{
	jiffies_64 += ticks;
	update_wall_time();
	calc_global_load();
}

void update_process_times(int user_tick) /* user_tick is set by looking at the system's registers (line 88) */
{
	struct task_struct* p = current;
	int cpu = smp_processor_id();

	account_process_tick(p, user_tick);
	run_local_timers();
	rcu_check_callbacks(cpu, user_tick);
	printk_tick();
	scheduler_tick();
	run_posix_cpu_timers(p);
}

void account_process_tick(struct task_struct* p, int user_tick) /* Does the actual updating of the process's times */
{
	cputime_t one_jiffy_scaled = cputime_to_scaled(cputime_one_jiffy);
	struct rq* rq = this_rq();
	
	if(user_tick)
		account_user_time(p, cputime_one_jiffy, one_jiffy_scaled);
	else if ((p != rq->idle) || (irq_count() != HARDIRQ_OFFSET))
		account_system_time(p, HARDIRQ_OFFSET, cputime_one_jiffy, one_jiffy_scaled);
	else
		account_idle_time(cputime_one_jiffy);
}
In reality, the process might have entered and exited kernel mode many times during the last tick. In fact, the process might not even have been the only process running in the last tick! This granular process accounting is classic unix, and without much more complex accounting, this is the best the kernel can provide.
Next, run_local_timers() marks a softirq to handle the execution of any expired timers.
Finally, the scheduler_tick() decrements the current running process's timeslice and sets need_resched if needed. On SMP machines, it also balances the per-processor runqueues as needed.
======================================================================

• The time of day:
The current time of day (the wall time) is defined in kernel/time/timekeeping.c:
=====================================================
struct timespec xtime;
The timespec structure is defined in <linux/time.h>: 
struct timespec{
	__kernel_time_t tv_sec; 	/* seconds that have elapsd since 01/01/1970 (UTC) */
	long_tv_nsec; 				/* nanoseconds that have elapsed in the last second */
};
=====================================================
To update xtime, a write seqlock is required:
write_seqlock(&xtime_lock);
/* update xtime... */
write_sequnlock(&xtime_lock);

Reading xtime requires the use of read_seqbegin() and read_seqretry(): 
unsigned long seq;
do{
	unsigned long lost;
	seq = read_seqbegin(&xtime_lock);

	usec = timer->get_offset();
	lost = jiffies - wall_jiffies;
	if(lost)
		usec += lost * (1000000 / HZ);
	sec = xtime.tv_sec;
	usec += xtime.tv_nsec / 1000;
}while(read_seqretry(&xtime_lock, seq));
This loop repeats until the reader is assured that it read the data without an intervening write. If the timer interrupt occurred and updated xtime during the loop, the returned sequence number is invalid and the loop repeats.

4. Timers:
Timers, also called dynamic timers or kernel timers, are used to delay executing work for a specified amount of time. Timers are constantly created and destroyed, and there's no limit on the number of timers.

• Using timers:
=======================================
Defined in <linux/timer.h>
struct timer_list{
	struct list_head entry;           /* entry in linked list of timers */
	unsigned long expires;            /* expiration value, in jiffies */
	void (*function)(unsigned long);  /* the timer handler */
	unsigned long data;               /* lone argument to the handler */
	struct tvec_t_base_s* base;       /* internal timer field, do not touch */
};
=======================================
struct timer_list my_timer;
init_timer(&my_timer);
my_timer.expires = jiffies + delay; /* expires in delay ticks */
my_timer.data = 0;
my_timer.function = my_function;
When the current jiffies is equal to or greater than my_timer.expires, the handler function my_function is run with the long argument my_timer.data. If you do not need the argument, you can simply pass 0 or any other value.
Finally, you activates the timer:
add_timer(&my_timer);
Note that timers cannot be used to implement any sort of hard real-time processing.

• Sometimes you might need to modify the expiration of an already active timer. The kernel implements a function: mod_timer(), which changes the expiration of a given timer:
mod_timer(&my_timer, jiffies + new_delay);
mod_timer() can operate on timers that are initialized but not active, then mod_timer() will activate it. It returns 0 if the timer is inactive originally, and returns 1 if the timer were active. In either case, upon return from mod_timer(), the timer is activated and set to the new expiration.

• If you need to deactivate a timer prior to its expiration, use del_timer():
	del_timer(&my_timer);
It works for both active and inactive timers. If the timer is already inactive, it return 0; otherwise it returns 1. There's no need to call this for timers that have expired because they are automatically deactivated.
A potential race condition exists when deleting timers. When del_timer() returns, it guarantees only that the timer is no longer active(it will not be executed in the future). But on a multiprocessing machine, the timer handler might already be executing on another processor. To deactivate the timer and wait until a potentially executing handler for the timer exits, use del_timer_sync():
	del_timer_sync(&my_timer);
Unlike del_timer(), del_timer_sync() cannot be used from interrupt context.

• Timer race conditions:
i)  Never do the following as a subsitute for a mere mod_timer() because this is unsafe on multiprocessing machines:
	del_timer(&my_timer);
	my_timer.expires = jiffies + new_delay;
	add_timer(&my_timer);

ii) In almost all cases, you should use del_timer_sync() over del_timer(). Otherwise, you cannot assume the timer is not currently running.

• Timer implementation:
The kernel executes timers in bottom-half context, as softirqs, after the timer interrupt completes. The timer interrupt handler runs update_process_times(), which calls run_local_timers():
void run_local_timers(void)
{
	hrtimer_run_queues();
	raise_softirq(TIMER_SOFTIRQ); /* raise the timer softirq */
	softlockup_tick();
}
The TIMER_SOFTIRQ softirq is handled by run_timer_softirq(). This function runs all the expired timers (if any) on the current processor.
Timers are stored in linked lists. However, the kernel partitions timers into 5 groups based on their expiration value. Timers move down through the groups as their expiration time draws closer. The partitioning makes the timer management code efficient although using linked lists.

5. Delaying execution:
The kernel provides a number of solutions. Some hog the processor while delaying; other solutions do not hog the processor but offer no guarantee that the code will resume in exactly the required time.

i)  Busy looping:
The simplest solution to implement delayed execution is busy looping/busy waiting. This technique works only when the time you want to delay is some integer multiple of the tick rate or precision is not important.
The idea is simple: spinning in a loop until the desired number of clock ticks pass. For example:
unsigned long timeout = jiffies + 10; // 10 ticks
while(time_before(jiffies, timeout))
	;
A better solution would be to reschedule your process to allow the processor to accomplish other work while your code waits:
unsigned long delay = jiffies + 5 * HZ;
while(time_before(jiffies, delay))
	cond_resched();
The call to cond_resched() schedules a new process, but only if need_resched is set. In other words, this solution conditionally invokes the scheduler only if there's more important task to run. Because this approach invokes the scheduler so you cannot use it from an interrupt handler. And all these approaches are best used from process context, because interrupt handlers should execute as quickly as possible.

ii) Small delays:
The kernel provides 3 functions  for microsecond, nanosecond, and millisecond delay, defined in <linux/delay.h> and <asm/delay.h>, which do not use jiffies:
void udelay(unsigned long usecs);
void ndelay(unsigned long nsecs);
void mdelay(unsigned long msecs);
The udelay is implemented as a busy loop that knows how many iterations can be executed in a given period of time. The mdelay() function is then implemented in terms of udelay(). Because the kernel knows how many loops the processor can complete in a second, the udelay() function simply scales that value to the correct number of loop iterations for the given delay.
(The BogoMIPS value is the number of busy loop iterations the processor can perform in a given period (in a given jiffy). The value is read from /proc/cpuinfo/bogomips and stored in the loops_per_jiffy variable. The delay loop functions use the loops_per_jiffy value to figure out fairly precisely how many busy loop iterations they need to execute to provide the requisite delay.)

The udelay() function should be called only for small delays because larger delays on fast machines might result in overflow. As a rule, do not use udelay() for delays more than 1 millisecond in duration. For larger durations, mdelay() works fine. Like other busy waiting solutions for delaying execution, neither of these functions should be used unless absolutely needed. Remember that it is rude to busy loop with locks held or interrupts disabled because system response and performance will be adversely affected. If you require precise delays, however, these calls are your best bet.

iii) schedule_timeout():
This call puts your task to sleep until at least the specified time has elapsed. There is no guarantee that the sleep duration will be exactly the specified time - only that the duration is at least as long as specified. When the specified time has elapsed, the kernel wakes the task up and places it back on the runqueue. The usage is easy:
	/* set task's state to interruptible sleep */
	set_current_state(TASK_INTERRUPTIBLE);
	/* take a nap and wake up in 's' seconds */
	schedule_timeout(s * HZ);
The lone parameter is the desired relative timeout, in jiffies. The task must be in state TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE before schedule_timeout() is called or else the task will not go to sleep. 
Because schedule_timeout() invokes the scheduler, so it cannot be used in interrupt context. In case the task was awakened prematurely, the timer is destroyed, the function then returns the time left (before timeout).
The MAX_SCHEDULE_TIMEOUT check enables a task to sleep indefinitely.

• Sleeping on a wait queue, with a timeout
Sometimes it is desirable to wait for a specific event or wait for a specified time to elapse - whichever comes first. In those cases, code might simply call: schedule_timeout() instead of schedule() after placing itself on a wait queue. The task wakes up when the desired event occurs or the specified time elapses. 
